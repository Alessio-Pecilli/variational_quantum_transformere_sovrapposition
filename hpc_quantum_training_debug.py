#!/usr/bin/env python3
"""
Versione HPC robusta con debug esteso
Gestisce crash e identifica problemi specifici dell'ambiente HPC
"""

import traceback
import sys
import time
import logging
from pathlib import Path
import os
import signal
from contextlib import contextmanager
import numpy as np
from multiprocessing import Pool, cpu_count
from functools import partial

# Setup logging dettagliato
def setup_logging():
    """Setup logging con timestamp e dettagli"""
    log_file = Path("hpc_debug.log")
    
    # Configurazione logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info("ðŸš€ Avvio logging dettagliato HPC")
    return logger

def log_system_info(logger):
    """Log informazioni sistema e ambiente"""
    logger.info("=" * 60)
    logger.info("ðŸ–¥ï¸ INFORMAZIONI SISTEMA HPC")
    logger.info("=" * 60)
    
    # Variabili ambiente importanti
    env_vars = ['OMP_NUM_THREADS', 'SLURM_CPUS_PER_TASK', 'SLURM_JOB_ID', 
                'SLURM_NTASKS', 'PBS_NP', 'SLURM_NPROCS']
    
    for var in env_vars:
        value = os.environ.get(var, 'Non definita')
        logger.info(f"   {var}: {value}")
    
    # Info Python e sistema
    logger.info(f"   Python version: {sys.version}")
    logger.info(f"   CPU count (physical): {cpu_count()}")
    logger.info(f"   Working directory: {os.getcwd()}")
    
    # Memoria (se disponibile)
    try:
        import psutil
        memory = psutil.virtual_memory()
        logger.info(f"   Memoria totale: {memory.total / (1024**3):.1f} GB")
        logger.info(f"   Memoria disponibile: {memory.available / (1024**3):.1f} GB")
    except ImportError:
        logger.info("   Memoria: Info non disponibile (psutil non installato)")

@contextmanager
def timeout_context(seconds, logger):
    """Context manager per timeout operazioni"""
    def timeout_handler(signum, frame):
        logger.error(f"â° TIMEOUT dopo {seconds} secondi!")
        raise TimeoutError(f"Operazione timeout dopo {seconds}s")
    
    # Setup timeout solo su Unix (HPC)
    if hasattr(signal, 'SIGALRM'):
        old_handler = signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(seconds)
    
    try:
        yield
    finally:
        if hasattr(signal, 'SIGALRM'):
            signal.alarm(0)
            signal.signal(signal.SIGALRM, old_handler)

def safe_import(module_name, logger):
    """Import sicuro con logging dettagliato"""
    try:
        logger.info(f"ðŸ“¦ Importazione {module_name}...")
        if module_name == "config":
            from config import TRAINING_SENTENCES, OPTIMIZATION_CONFIG
            logger.info(f"   Config caricato: {len(TRAINING_SENTENCES)} sentences")
            return True
        elif module_name == "encoding":
            from encoding import Encoding
            logger.info("   Encoding class caricata")
            return True
        elif module_name == "quantum_circuits":
            from quantum_circuits import get_circuit_function
            logger.info("   Quantum circuits caricati")
            return True
        elif module_name == "quantum_mpi_utils":
            from quantum_mpi_utils import loss_and_grad_for_sentence
            logger.info("   MPI utils caricati")
            return True
        elif module_name == "main_superposition":
            from main_superposition import process_sentence_states
            logger.info("   Main superposition caricato")
            return True
        else:
            exec(f"import {module_name}")
            logger.info(f"   {module_name} importato con successo")
            return True
            
    except Exception as e:
        logger.error(f"âŒ ERRORE import {module_name}: {type(e).__name__}: {str(e)}")
        logger.error("Traceback:")
        for line in traceback.format_exc().splitlines():
            logger.error(f"   {line}")
        return False

def get_hpc_workers():
    """Auto-detect workers per HPC (copia da hpc_quantum_training.py)"""
    omp = os.environ.get('OMP_NUM_THREADS')
    slurm = os.environ.get('SLURM_CPUS_PER_TASK')
    pbs = os.environ.get('PBS_NP')
    
    if omp and int(omp) > 1: 
        workers = int(omp)
        source = "OMP_NUM_THREADS"
    elif slurm and int(slurm) > 1: 
        workers = int(slurm)
        source = "SLURM_CPUS_PER_TASK"
    elif pbs and int(pbs) > 1: 
        workers = int(pbs)
        source = "PBS_NP"
    else:
        workers = cpu_count()
        source = "CPU_COUNT"
    
    return workers

def get_safe_worker_count(logger):
    """Determina numero workers sicuro per HPC"""
    try:
        detected_workers = get_hpc_workers()
        logger.info(f"ðŸ“Š Workers rilevati automaticamente: {detected_workers}")
        
        # Limiti di sicurezza per HPC
        max_safe_workers = min(detected_workers, 16)  # Limite cautelativo
        
        logger.info(f"ðŸ“Š Workers scelti per sicurezza: {max_safe_workers}")
        return max_safe_workers
        
    except Exception as e:
        logger.error(f"âŒ Errore rilevamento workers: {e}")
        logger.info("ðŸ”„ Fallback a 4 workers")
        return 4

def test_single_calculation(logger):
    """Test calcolo singolo prima del multiprocessing"""
    try:
        logger.info("ðŸ§ª TEST: Calcolo singolo...")
        
        from config import TRAINING_SENTENCES, OPTIMIZATION_CONFIG
        from encoding import Encoding
        from quantum_mpi_utils import loss_and_grad_for_sentence
        
        # USA LA STESSA LOGICA DI main_superposition.py
        sentences = TRAINING_SENTENCES
        sentence = sentences[0]  # Prima sentence per test
        logger.info(f"   Sentence: '{sentence}'")
        
        # Encoding come nel main_superposition.py
        encoding = Encoding(sentences, embeddingDim=OPTIMIZATION_CONFIG['embedding_dim'])
        logger.info(f"   Encoding creato: {len(encoding.stateVectors)} state vectors")
        
        # Parametri come nel main_superposition.py
        from quantum_utils import get_params
        param_shape = get_params(OPTIMIZATION_CONFIG['num_qubits'], OPTIMIZATION_CONFIG['num_layers']).shape
        n_params = int(np.prod(param_shape))
        num_params = 2 * n_params  # V and K parameters
        params = np.random.randn(num_params) * 0.1
        sentence_data = (sentence, encoding, len(sentence.split()))
        
        logger.info(f"   Parametri: {num_params} (V:{n_params}, K:{n_params}, shape: {params.shape})")
        
        # Test calcolo gradiente parallelo come nel tuo MPI
        with timeout_context(30, logger):
            from main_superposition import process_sentence_states
            from quantum_circuits import get_circuit_function
            
            # Process states come nel codice funzionante
            states = encoding.stateVectors[0]  # Prima sentence
            states_calculated, U, Z = process_sentence_states(states)
            logger.info(f"   States calculated: {len(states_calculated)}")
            
            if len(states_calculated) > 0:
                # Setup per calcolo gradiente COME NEL MAIN
                circuit_func = get_circuit_function(len(states_calculated))
                psi = states_calculated[0]  # Primo stato per test
                U_test = [U[0]] if U else None  # Keep as list
                Z_test = [Z[0]] if Z else None  # Keep as list
                
                # Test calcolo loss singolo
                from quantum_mpi_utils import _compute_single_gradient_component
                shift = np.pi / 2
                
                # Test gradiente per primo parametro
                grad_comp = _compute_single_gradient_component(
                    0, params, shift, states_calculated, U, Z, 
                    OPTIMIZATION_CONFIG['num_layers'], 
                    OPTIMIZATION_CONFIG['embedding_dim'], 
                    circuit_func
                )
                logger.info(f"   âœ… Gradiente componente 0: {grad_comp:.6f}")
            else:
                logger.info("   âš ï¸ Nessun state calcolato per il test gradiente")
            
        logger.info(f"   âœ… Test gradiente OK")
        return True, (states_calculated, U, Z, sentence, encoding, params)
        
    except Exception as e:
        logger.error(f"âŒ ERRORE nel test singolo: {type(e).__name__}: {str(e)}")
        return False, None

def test_multiprocessing_safe(workers, test_data, logger):
    """Test multiprocessing con gestione errori"""
    try:
        logger.info(f"ðŸ”¥ TEST: Multiprocessing con {workers} workers...")
        
        from quantum_circuits import get_circuit_function
        from quantum_mpi_utils import _compute_single_gradient_component
        from config import OPTIMIZATION_CONFIG
        states_calculated, U, Z, sentence, encoding, params = test_data
        
        if len(states_calculated) == 0:
            logger.error("   âŒ Nessun state calcolato, impossibile testare gradiente parallelo")
            return False
        
        # Test con timeout piÃ¹ lungo
        with timeout_context(60, logger):
            with Pool(processes=workers) as pool:
                logger.info(f"   Pool creato con {workers} workers")
                
                # Test calcolo gradiente parallelo per primi N parametri
                circuit_func = get_circuit_function(len(states_calculated))
                psi = states_calculated[0]
                U_test = [U[0]] if U else None  # Keep as list
                Z_test = [Z[0]] if Z else None  # Keep as list
                shift = np.pi / 2
                
                # Parallelizza calcolo gradiente per primi 4 parametri
                n_test_params = min(4, len(params))
                logger.info(f"   Test gradiente parallelo per {n_test_params} parametri...")
                
                tasks = []
                for i in range(n_test_params):
                    task_args = (i, params, shift, states_calculated, U, Z,
                                OPTIMIZATION_CONFIG['num_layers'], 
                                OPTIMIZATION_CONFIG['embedding_dim'], 
                                circuit_func)
                    tasks.append(task_args)
                
                logger.info("   Invio tasks al pool...")
                results = pool.starmap(_compute_single_gradient_component, tasks)
                
                logger.info(f"   âœ… Completato: {len(results)} risultati")
                
                # Verifica risultati
                if len(results) == 2:
                    loss1, grad1 = results[0]
                    loss2, grad2 = results[1]
                    logger.info(f"   Loss1: {loss1:.6f}, Loss2: {loss2:.6f}")
                    logger.info(f"   Differenza loss: {abs(loss1-loss2):.8f}")
                
        return True
        
    except Exception as e:
        logger.error(f"âŒ ERRORE multiprocessing {workers} workers:")
        logger.error(f"   Tipo: {type(e).__name__}")
        logger.error(f"   Messaggio: {str(e)}")
        
        # Prova con meno workers
        if workers > 2:
            reduced_workers = max(2, workers // 2)
            logger.info(f"ðŸ”„ Retry con {reduced_workers} workers...")
            return test_multiprocessing_safe(reduced_workers, test_data, logger)
        else:
            logger.error("âŒ Multiprocessing fallito anche con 2 workers")
            return False

def main():
    """Main con gestione errori completa"""
    logger = setup_logging()
    
    try:
        logger.info("ðŸš€ AVVIO PROGRAMMA HPC CON DEBUG ESTESO")
        log_system_info(logger)
        
        # Step 1: Importazioni
        logger.info("\n" + "="*40)
        logger.info("STEP 1: IMPORTAZIONI")
        logger.info("="*40)
        
        modules = ["numpy", "config", "encoding", "quantum_circuits", "quantum_mpi_utils", "main_superposition"]
        for module in modules:
            if not safe_import(module, logger):
                logger.error(f"ðŸ’¥ STOP: Impossibile importare {module}")
                return 1
        
        # Step 2: Rilevamento workers
        logger.info("\n" + "="*40)
        logger.info("STEP 2: CONFIGURAZIONE WORKERS")
        logger.info("="*40)
        
        workers = get_safe_worker_count(logger)
        
        # Step 3: Test singolo
        logger.info("\n" + "="*40)
        logger.info("STEP 3: TEST CALCOLO SINGOLO")
        logger.info("="*40)
        
        success, test_data = test_single_calculation(logger)
        if not success:
            logger.error("ðŸ’¥ STOP: Test singolo fallito")
            return 1
        
        # Step 4: Test multiprocessing
        logger.info("\n" + "="*40)
        logger.info("STEP 4: TEST MULTIPROCESSING")
        logger.info("="*40)
        
        if not test_multiprocessing_safe(workers, test_data, logger):
            logger.error("ðŸ’¥ STOP: Multiprocessing fallito")
            return 1
        
        # Step 5: Esecuzione training (se test OK)
        logger.info("\n" + "="*40)
        logger.info("STEP 5: TRAINING REALE")
        logger.info("="*40)
        
        # Import per training
        from config import TRAINING_SENTENCES, OPTIMIZATION_CONFIG
        from encoding import Encoding
        from main_superposition import process_sentence_states
        
        logger.info(f"âš¡ Inizio training con {workers} workers...")
        logger.info(f"ðŸ“Š Configurazione: {OPTIMIZATION_CONFIG}")
        
        # USA LA STESSA LOGICA DI main_superposition.py
        sentences = TRAINING_SENTENCES
        sentence = sentences[0]  # Prima sentence per test
        encoding = Encoding(sentences, embeddingDim=OPTIMIZATION_CONFIG['embedding_dim'])
        
        logger.info(f"ðŸ“ˆ Test sentence: '{sentence}' ({len(sentence.split())} parole)")
        logger.info(f"   State vectors disponibili: {len(encoding.stateVectors)}")
        
        # Test process_sentence_states 
        states = encoding.stateVectors[0]
        states_calculated, U, Z = process_sentence_states(states)
        
        logger.info(f"   âœ… States calculated: {len(states_calculated)}")
        logger.info(f"   âœ… U matrices: {len(U)}")  
        logger.info(f"   âœ… Z matrices: {len(Z)}")
        
        # TEST CALCOLO GRADIENTE PARALLELO (OBIETTIVO FINALE)
        if len(states_calculated) > 0:
            from quantum_circuits import get_circuit_function
            from quantum_mpi_utils import _compute_single_gradient_component
            
            circuit_func = get_circuit_function(len(states_calculated))
            psi = states_calculated[0]
            U_test = [U[0]] if U else None  # Keep as list
            Z_test = [Z[0]] if Z else None  # Keep as list
            shift = np.pi / 2
            
            # Parametri per test
            from quantum_utils import get_params
            param_shape = get_params(OPTIMIZATION_CONFIG['num_qubits'], OPTIMIZATION_CONFIG['num_layers']).shape
            n_params = int(np.prod(param_shape))
            num_params = 2 * n_params  # V and K parameters
            params = np.random.randn(num_params) * 0.1
            
            with Pool(processes=workers) as pool:
                logger.info(f"   ðŸ”¥ GRADIENTE PARALLELO con {workers} workers")
                logger.info(f"      Parametri totali: {num_params}")
                
                # Parallelizza calcolo gradiente per tutti i parametri
                n_test_params = min(8, num_params)  # Test con primi 8 parametri
                logger.info(f"      Test su {n_test_params} parametri...")
                
                tasks = []
                for i in range(n_test_params):
                    task_args = (i, params, shift, states_calculated, U, Z,
                                OPTIMIZATION_CONFIG['num_layers'], 
                                OPTIMIZATION_CONFIG['embedding_dim'], 
                                circuit_func)
                    tasks.append(task_args)
                
                start_time = time.time()
                grad_components = pool.starmap(_compute_single_gradient_component, tasks)
                elapsed = time.time() - start_time
                
                logger.info(f"   âœ… GRADIENTE PARALLELO COMPLETATO in {elapsed:.2f}s")
                logger.info(f"      Componenti calcolate: {len(grad_components)}")
                logger.info(f"      Gradiente norm: {np.linalg.norm(grad_components):.6f}")
                logger.info(f"   ðŸŽ¯ PARALLELIZZAZIONE GRADIENTE FUNZIONA!")
                
                # ðŸš€ ORA FACCIAMO IL VERO TRAINING SU TUTTE LE FRASI!
                logger.info(f"\nðŸŽ¯ AVVIO TRAINING VERO SU TUTTE LE {len(sentences)} FRASI!")
                
                # Calcola loss iniziale per confronto
                current_loss = circuit_func(
                    states_calculated, U, Z,
                    params[:num_params//2].reshape(param_shape),
                    params[num_params//2:].reshape(param_shape),
                    OPTIMIZATION_CONFIG['num_layers'], 
                    OPTIMIZATION_CONFIG['embedding_dim']
                )
                logger.info(f"   Loss iniziale (prima frase): {current_loss:.6f}")
                
                # Prepara tutti i dati di training
                all_training_data = []
                for i, sent in enumerate(sentences):
                    states_i = encoding.stateVectors[i]
                    states_calc_i, U_i, Z_i = process_sentence_states(states_i)
                    if len(states_calc_i) > 0:
                        all_training_data.append((states_calc_i, U_i, Z_i, sent))
                        logger.info(f"   Frase {i+1}: '{sent[:30]}...' -> {len(states_calc_i)} states")
                
                logger.info(f"ðŸ“Š DATASET: {len(all_training_data)} frasi valide per training")
                
                if len(all_training_data) > 0:
                    # Parametri per training vero
                    learning_rate = 0.01
                    max_epochs = 10  # Epoch su tutto dataset
                    
                    # Parametri iniziali casuali
                    best_loss = float('inf')
                    best_params = params.copy()
                    
                    logger.info(f"ðŸš€ TRAINING: {max_epochs} epochs, LR={learning_rate}")
                    
                    # Training loop su tutto il dataset - CORRETTO!
                    for epoch in range(max_epochs):
                        epoch_start = time.time()
                        total_gradient = np.zeros(num_params)  # Gradiente completo
                        epoch_loss = 0.0
                        
                        logger.info(f"   Epoca {epoch}: Calcolo gradiente completo su {num_params} parametri...")
                        
                        # GRADIENT ACCUMULATION: Somma gradienti da tutte le frasi
                        for batch_idx, (states_calc, U_batch, Z_batch, sent) in enumerate(all_training_data):
                            # Calcola loss per questa frase
                            loss_batch = circuit_func(
                                states_calc, U_batch, Z_batch,
                                params[:num_params//2].reshape(param_shape),
                                params[num_params//2:].reshape(param_shape),
                                OPTIMIZATION_CONFIG['num_layers'], 
                                OPTIMIZATION_CONFIG['embedding_dim']
                            )
                            epoch_loss += loss_batch
                            
                            # Calcola TUTTO il gradiente per questa frase
                            grad_tasks = []
                            for j in range(num_params):  # TUTTI i parametri
                                grad_tasks.append((j, params, shift, states_calc, U_batch, Z_batch,
                                                 OPTIMIZATION_CONFIG['num_layers'], 
                                                 OPTIMIZATION_CONFIG['embedding_dim'], 
                                                 circuit_func))
                            
                            # Calcola gradiente completo in parallelo
                            grad_batch = np.array(pool.starmap(_compute_single_gradient_component, grad_tasks))
                            
                            # ACCUMULA gradiente (non aggiorna subito)
                            total_gradient += grad_batch
                            
                            logger.info(f"      Frase {batch_idx+1}/{len(all_training_data)}: Loss={loss_batch:.4f}")
                        
                        # AGGIORNAMENTO PARAMETRI: Una volta per epoca con gradiente medio
                        avg_gradient = total_gradient / len(all_training_data)
                        params -= learning_rate * avg_gradient  # Aggiorna TUTTI i parametri
                        
                        # Media loss su tutto dataset
                        avg_loss = epoch_loss / len(all_training_data)
                        epoch_time = time.time() - epoch_start
                        
                        # Tracking miglior modello
                        if avg_loss < best_loss:
                            best_loss = avg_loss
                            best_params = params.copy()
                            logger.info(f"   ðŸŽ¯ NEW BEST! Epoch {epoch}: Avg Loss={avg_loss:.6f}")
                        
                        logger.info(f"   Epoch {epoch:2d}: Avg Loss={avg_loss:.6f} Time={epoch_time:.2f}s")
                        
                        # Early stopping se loss non migliora
                        if avg_loss < 0.01:  # Soglia di convergenza
                            logger.info(f"   âœ… CONVERGENZA! Loss < 0.01 raggiunta")
                            break
                    
                    # Salva risultati finali
                    final_improvement = ((current_loss - best_loss) / current_loss) * 100  # % riduzione loss
                    logger.info(f"ðŸŽ‰ TRAINING COMPLETATO!")
                    logger.info(f"   Loss iniziale: {current_loss:.6f}")  
                    logger.info(f"   Loss finale: {avg_loss:.6f}")
                    logger.info(f"   Miglior loss: {best_loss:.6f}")
                    logger.info(f"   Riduzione loss: {final_improvement:.1f}%")
                    
                    # Analisi performance
                    if best_loss < 0.1:
                        logger.info(f"   âœ… OTTIMO: Loss < 0.1 raggiunta!")
                    elif best_loss < 0.3:
                        logger.info(f"   ðŸ‘ BUONO: Loss decente, ma puÃ² migliorare")
                    else:
                        logger.info(f"   âš ï¸  ATTENZIONE: Loss ancora alta, serve piÃ¹ training")
                        
                    logger.info(f"   ðŸ“Š Frasi processate: {len(all_training_data)}")
                    logger.info(f"   âš¡ Stati totali: {sum(len(data[0]) for data in all_training_data)}")
                    
                    # Salva parametri ottimali
                    import pickle
                    param_file = f"best_params_hpc_{time.strftime('%Y%m%d_%H%M%S')}.pkl"
                    with open(param_file, 'wb') as f:
                        pickle.dump({
                            'best_params': best_params,
                            'best_loss': best_loss,
                            'final_loss': avg_loss,
                            'config': OPTIMIZATION_CONFIG,
                            'num_sentences': len(all_training_data)
                        }, f)
                    logger.info(f"   ðŸ’¾ Parametri salvati: {param_file}")
                else:
                    logger.error("   âŒ Nessun dato valido per training!")
                    
        else:
            logger.error("   âŒ Nessun state calcolato per il test gradiente")
        
        logger.info("ðŸš€ TRAINING HPC COMPLETATO SU TUTTE LE FRASI!")
        return 0
        
    except Exception as e:
        logger.error(f"\nðŸ’¥ ERRORE GENERALE DEL PROGRAMMA:")
        logger.error(f"Tipo: {type(e).__name__}")
        logger.error(f"Messaggio: {str(e)}")
        logger.error("Traceback completo:")
        for line in traceback.format_exc().splitlines():
            logger.error(f"  {line}")
        return 1

if __name__ == "__main__":
    exit_code = main()
    print(f"\nðŸ Programma terminato con exit code: {exit_code}")
    sys.exit(exit_code)